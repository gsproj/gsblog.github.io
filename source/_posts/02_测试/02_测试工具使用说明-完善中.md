---
title: 测试工具使用说明-完善中
date: 2022-07-06 11:19:52
categories:
- 测试
- 信创POC测试
tags:
- 工作
---

# 测试工具使用说明-汇总

# 一、SPEC CPU 2017 测试

## 1.1 概述

​	标准性能评测机构（SPEC）开发的用于评测CPU性能的基准程序测试组，是一套CPU子系统测试工具。处理器、内存和编译都会影响最终的测试结果，目前SPEC CPU是业界首选的CPU评测工具。 

## 1.2 工具安装

获取镜像包：

​	cpu2017-1_0_5.iso

挂载IOS

```shell
mount -o loop -t iso9660 cpu2017-1_0_5.iso /xxx/cpu2017CD
```

安装工具

```shell
cd /xxx/cpu2017CD
./install.sh
```

## 1.3 测试步骤

spec2017主要分为四项测试：

- ​	intrate
- ​	fprate
- ​	intspeed
- ​	fpspeed

​	根据需要测试的cpu型号，可以到官网下载相应的cfg文件，修改icc、Jemalloc库、qkmalloc库的路径，放入config文件夹中。

测试命令如下：

```shell
source shrc
ulimit -s unlimited
# speed 测试
runcpu --config=icc-speed-official.cfg --threads=48 --define cores=48 -n 3 -i ref fpspeed intspeed

# rate 测试
runcpu --config=icc-rate-official.cfg -copies=48 -n 3 -i ref intrate fprate

-----------------------------------------------------------------------

# 仅编译不跑测试
--action=build
# 重新编译
--rebuild
# 绑核跑
taskset -c 
```

## 1.4 测试优化

>Bios设置：
>
>​	开启超线程
>
>​	 尝试开启LLC-PREFETCH

## 1.5 其他事项：

```shell
cfg文件中的submit可以设置绑核选项
```

# 二、SPEC CPU 2006 测试

## 2.1 概述

​	作用同Spec CPU 2017, 近年来逐渐被淘汰

## 2.2 测试步骤

1、spec2006.tgz 解压，使用install.sh安装

```shell
SPEC CPU2006 Installation
Top of the CPU2006 tree is '/vol8/tarball/cpu2006'
These appear to be valid toolsets:
ft-spec2006-tool
aarch64-linux
Enter the architecture you are using:
>> aarch64-linux
```

```shell
source shrc
# 单核测试
runspec -c linux64-arm64-gcc52.cfg -i ref -n 3 -I all 
# 多核测试（16）
runspec -c linux64-arm64-gcc52.cfg -r 16 -n 3 -i ref -I all
```

# 三、 SPEC OMP 2012 测试

## 3.1 概述

​	基于SPEC测试套件的OpenMP评测工具，其中包含15个基于OpenMP的并行程序。

## 3.2 工具安装

​	获取源包：

​		omp2012-1.1.zip

​	解压后使用install.sh安装

```shell
# 安装
./install.sh -d /home/xx
# 仅编译
runspec --action=build --config=gcc.cfg -i ref -I all
```

## 3.3 测试步骤

```shell
source shrc
ulimit -s unlimited
export OMP_NUM_THREADS=48
runspec --config=gcc.cfg --threads 48 -n 3 -i ref -I all
```

# 四、NPB 测试

## 4.1 概述

​	NAS并行基准测试程序（NPB），是由美国航空航天局开发的一套代表流体动力学计算的应用程序集，它已经成为公认的用于测评大规模并行机和超级计算机的标准测试程序。NPB可用于常用的编程模型，如MPI和OpenMP。

## 4.2 工具安装

源包获取：

​	NPB3.4.tar.gz         OpenMP || MPICH

​	NPB3.4-MZ.tar.gz  OpenMP && MPICH

解压即可。

## 4.3 测试步骤

```shell
make suite
cd bin
mpirun -np 4 ./xxx
```

# 五、Stream 测试

## 5.1 概述

​	STREAM是一套综合性能测试程序集，通过fortran和C两种高级且高效的语言编写完成，由于这两种语言在数学计算方面的高效率， 使得 STREAM 测试例程可以充分发挥出内存的能力。Stream测试是内存测试中业界公认的内存带宽性能测试基准工具。

## 5.2 工具安装

源包获取：

​	stream.tgz  刘新娃修改过

​	<font color='red'>Intel平台请用ICC</font>

解压后编译stream.c, 生成可执行文件

```shell
icc -O3 -fopenmp -DNTIMES=10 -mcmodel=large -o stream_c stream.c
```

没有修改过的怎么编译呢？

```shell
icc -mtune=native -march=native -O3 -mcmodel=large -fopenmp -DSTREAM_ARRAY_SIZE=100000000 -DNTIMES=30 -DOFFSET=512 stream.c -o stream-gs
```

## 5.3 测试步骤

```shell
export OMP_NUM_THREADS=48/32/16/8/4  # 设置每个进程的线程数
./stream_c 200000 # 使用200G内存
```

## 5.4 其他事项

>`STREAM_ARRAY_SIZE`即`N`指定计算中a[],b[],c[]三个数组的大小，且数组的值采用了双精度（8个字节）。数组的维数 N定义时需要注意以下几点：
>
>一、要充分考虑内存容量的需求，粗略估计是 N× 8（双精度浮点类型） × 3 （三个数组）<= 0.6*M；M 是用户的可用内存。
>
>二、要保证测试过程中，使用到的内存容量要大于处理器内的缓存，只有这样才会有内存的操作，而不仅仅是对处理器内缓存的操作。
>
>三、为了保证测试可以持续一段时间，测试过程中内存带宽可以达到一定的最大值， 从而避免得不到实际最大峰值的情况，如果四项测试中有完成时间小于20微秒的情况，就需要适当的增大测试数组的维度 N。

# 六、Linpack 测试

## 6.1 概述

​	Linpack是国际上使用最广泛的测试高性能计算机系统浮点性能的基准测试。通过对高性能计算机采用高斯消元法求解一元 N次稠密线性代数方程组的测试，评价高性能计算机的浮点计算性能。Linpack的结果按每秒浮点运算次数（flops）表示。

```shell
 N ^ 2 * 8 = 总内存
```

## 6.2 工具安装

源包获取：

​	linpack.tgz

解压即可。

## 6.3 测试步骤

```shell
# 1进48线100G内存
./xhpl -n 1 -b 384 -p 1 -q 1 -m 100000
# 1进48线200G内存
./xhpl -n 1 -b 384 -p 1 -q 1 -m 200000
# 2进24线共100G内存
mpirun -n 2 ./xhpl -b 384 -p 1 -q 2 -m 50000
# 2进24线共200G内存
mpirun -n 2 ./xhpl -b 384 -p 1 -q 2 -m 100000
```

用mpirun提交xhplrun.sh效果更好

```shell
PRO_SIZE=${PMI_SIZE}
threads=`echo ${Cores}/${PMI_SIZE} | bc`
PMI_RANK_my=${PMI_RANK}
```

## 6.4 测试优化（重点）

>一、计算理论内存总带宽
>
>```shell
>计算方法：
>查看内存带宽：
>dmidecode | grep -A 16 "Memory Device"
>例如内存信息为：
>三星 DDR4 
>16G 每根
>2666 MT/s（Mhz）
>查看cpu支持的通道数，例如intel 6252N支持6通道，两个CPU支持12通道
>理论带宽计算DDR4：
>单条内存带宽 = 内存核心频率 x 内存总线位数 x 倍增系数
>	 = 2666 * 64 / 8 = 21328 MB/s = 21.3G/s
>12通道总内存带宽 = 21328 * 12 = 255936 = 250G/s
>```
>
>二、使用stream测试实际内存总带宽
>
>​	如果可以达到90%或以上，说明内存带宽正常, 继续Linpack测试
>
>三、计算CPU理论峰值
>
>```shell
>计算方法：
>	Mhz * 每个时钟周期执行浮点运算的次数 * CPU数目
>=
>	2.3 x ( 8 x 2 x 2 ) x 48 = 3532.8 Gflops
>说明：
>	峰值计算分为单精度和双精度浮点运算
>单精度：
>	32bit的指令长度的运算，对应32位操作系统
>双精度：
>	64bit的指令长度的运算，对应64位操作系统
>查找CPU可以处理什么样的指令集：
>	例如Intel官网查到Intel Xeon 6252N
>	支持AVX-512，
>	# of AVX-512 FMA Units = 2
>	(Fused Multiply Add instructions) 融合了 乘法 和 加法
>	即可以单个周期同时执行2条512bit的加法和2条512bit的乘法
>理解上述两个概念，可以开始计算（CPU单周期浮点计算能力）
>Intel 6252N (支持avx512，有512位)
>单精度：
>	2.3 x 512/32 x 2 x 2 = 147.2Gflops 
>双精度
>	2.3 x 512/64 x 2 x 2 = 73.6Gflops
>双精度共48核
>	74.6 x 48 = 3580
>	
>FT2000+ 和 FT1500A (只有128位)，FT3000加入sve指令集(128-256位)
>单精度： 
>	2.2 x 128/32 * 2 = 4.4
>双精度:
>	2.2 x 128/64 * 2 = 8.8
>双精度共64核
>	8.8 x 64 = 563.2
>```
>
>四、使用Linapack测试CPU实际峰值
>
>```shell
>如果性能达到理论峰值的70%说明正常，如果未达到尝试以下优化
>```
>
>```shell
>1、使用mpirun运行xhpl, 按`lscpu` 中的NUMA分组绑定,`比如48核，分为两个Numa Node`, 则用mpirun`跑两个xhpl进程`，每个进程使用 OMP_NUM_THREADS=24, 跑24线程，这样可以将核用满。
>在`Intel`平台，两个进程分别使用 HPL_HOST_CORE='0-23' 以及 HPL_HOST_CORE='24-47' 绑定核
>在`Arm64`平台，则使用 GOMP_CPU_AFFINITY='0-23' 以及 GOMP_CPU_AFFINITY='24-47' 绑定
>2、内存不用分配太大，根据stream测试结果分配，从小开始测
>3、Intel平台编译器使用ICC，测试linpack使用icc自带的linpack，mpi使用icc自带的
>4、Bios中关闭超线程，开启超频(turbo/P-state)
>```
>
>**<font color="blue">先测stream,判断带宽是否正常，再测linpack，然后再测Spec Cpu</font>**

```shell
# Intel平台可以使用的控制频率的命令
cpupower -c all frequency-set -g performance
cpupower -c 0-95 frequency-set -g userspace
cpupower -c 0-95 frequency-set -f 2700000

# 查看当前频率控制是否为perfomance？
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor

# 跑xhpl之前设置指令集，或许有用
export MKL_ENABLE_INSTRUCTIONS=AVX512
```

![image-20200415171716940](G:\工作\2020年3月\CN9性能测试\测试组合.png)

## 6.5 HPL.dat修改

```shell
# 第1-2行，注释说明行
HPLinpack benchmark input file   
Innovative Computing Laboratory, University of Tennessee  

# 第 3-4 行，输出结果文件的形式
HPL.out      output file name (if any)  
6            device out (6=stdout,7=stderr,file) 

# 5-6行，求解矩阵的大小
1            # of problems sizes (N)  
1000         Ns   

# 7-8行 求解矩阵分块的大小
1            # of NBs
192 256      NBs

# 9行 处理器阵列的排列方式，行还是列
0：适用于节点较少，单个节点CPU较多的胖系统
1：适用于节点较多，单个节点CPU较少的瘦系统（机群上远好于按行排列）
1            PMAP process mapping (0=Row-,1=Column-major) 

# 10-12行 定义二维处理网格P/Q
# P X Q = 进程数，P尽量小于Q
# P=2^n,P最好选择2的幂
1            # of process grids (P x Q) 
1 2          Ps
1 2          Qs

# 13行 设置阀值，不用修改
16.0         threshold

# 14-21行 设置L的分解方式
1            # of panel fact
2 1 0        PFACTs (0=left, 1=Crout, 2=Right) # 对性能影响不大
1            # of recursive stopping criterium
4 8          NBMINs (>= 1)   # 4或8都不错
1            # of panels in recursion
2            NDIVs  # 选2比较理想
1            # of recursive panel fact.
1 0 2        RFACTs (0=left, 1=Crout, 2=Right) # 对性能影响不大

# 22-23行 设置L的横向广播方式
# 前四种，适用于快速网络，后两种适用于速度较慢的网络
# 小规模系统，选择0/1，大规模系统选择3
1            # of broadcast
0            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)

# 24-25行 设置横向通信的通信深度
# 小规模系统选择1/2，大规模系统2-5之间
1            # of lookahead depth
1            DEPTHs (>=0)

# 26-27 设置U的广播算法
# 小规模系统，使用缺省值即可
0            SWAP (0=bin-exch,1=long,2=mix)
1            swapping threshold

# 28-29行 L和U的数据存放格式
0：按列存放
1：按行存放
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form

# 30-31 缺省值即可
0            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)

```

## 6.6 测试脚本编写

### 6.6.1 本地mpirun测试脚本（Intel版本）

```shell
# 本地单节点测试，需要根据`lscpu`中numa分区情况(node)的使用mpirun测试xhpl
# eg:在未开超线程的情况下，分为两个node，每个node中有12个核，则最佳测试方法为mpirun -np 2 ./myrun.sh
# 以下为myrun.sh的具体实现

#!/bin/bash
ulimit -s unlimited
Cores=48
ThreadsNum=`echo "${Cores}/${PMI_SIZE}" | bc` # 计算每个进程跑的线程数
case ${PMI_SIZE} in
2)
        case ${PMI_RANK} in
        0)
        export OMP_NUM_THREADS=${ThreadsNum}
        export HPL_HOST_CORE='0-23'
        ./xhpl -n 1 -b 384 -p ${1} -q ${2} -m ${3}
        ;;
        1)
        export OMP_NUM_THREADS=${ThreadsNum}
        export HPL_HOST_CORE='24-47'
        ./xhpl -n 1 -b 384 -p ${1} -q ${2} -m ${3}
        ;;
        esac
;;
esac

```

## 6.6.2 Slurm srun测试脚本(FT版本)

xhplrun.sh

```shell
#!/bin/bash
ulimit -s unlimited
Cores=48


==================================================================
PMI_SIZE=$SLURM_NPROCS
PMI_RANK=$SLURM_PROCID
MPI_NUM_NODE=$SLURM_NNODES
MPI_PER_NODE=$((PMI_SIZE / MPI_NUM_NODES))
MPI_RANK_FOR_NODE=$((PMI_RANK % MPI_PER_NODE))

PRO_SIZE=${MPI_PER_NODE}
PMI_RANK_my=${MPI_RANK_FOR_NODE}
==================================================================

echo ${PRO_SIZE} ${PMI_RANK_my} ${threads} ${PMI_SIZE} ${PMI_RANK} ${MPI_NUM_NODES} ${MPI_PER_NODE} ${MPI_RANK_FOR_NODE}

export HPL_CMDLINE=1
case ${PRO_SIZE} in
1)
#numactl -i 0-1 -N 0-1 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 0,4 -N 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0-63'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
2)
case ${PMI_RANK_my} in
0)
#numactl -i 0-3 -N 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -m 3 -N 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0-31'
numactl -i 4-7 -N 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
1)
#numactl -i 4-7 -N 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -m 7 -N 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='32-63'
numactl -i 0-3 -N 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
esac
#mpirun -n 1 numactl -i 0-3 -N 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3} : -n 1 numactl -i 4-7 -N 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
esac
```

yhrun 提交作业 runpro.sh

```shell
#duqi

if [ $# -ne 5 ]
then
        echo Usage: ./runpro.sh nodes_list nodes_num proc_per_node mem_per_proc logdir
        exit 0
fi

export HPL_CMDLINE=1

export LD_LIBRARY_PATH=/usr/local/mpi3/lib:$LD_LIBRARY_PATH

nodes=${1}
nnodes=${2}
nprocs=${3}
nmem=${4}
logdir=${5}


mkdir -p ${logdir}

tnprocs=$((${nnodes}*${nprocs}))
P=1
Q=1
for i in `seq 1 ${tnprocs}`
do
        for j in `seq 1 ${tnprocs}`
        do
                PQ=$((${i}*${j}))
                if [ ${PQ} -eq ${tnprocs} ] && [ ${i} -le ${j} ]
                then
                        P=${i}
                        Q=${j}
                fi
        done
done

for i in `yhcontrol show hostname ${nodes}`
do
        scp HPL.dat $i:/root/linpack &
done
wait
sleep 1

j=0
runnodes=''

for i in `yhcontrol show hostname ${nodes}`
do
        runnodes+=${i},
        let j++
        if [ ${j} -eq ${nnodes} ]
        then
                echo "yhrun -p all -N ${nnodes} -n ${tnprocs} -w ${runnodes} -D /root/linpack /root/linpack/xhplrun.sh ${P} ${Q} ${nmem} &>> ${logdir}/${i}.log &"
                echo ${runnodes} &>> ${logdir}/${i}.log
                yhrun -p all -N ${nnodes} -n ${tnprocs} -w ${runnodes} -D /root/linpack /root/linpack/xhplrun.sh ${P} ${Q} ${nmem} &>> ${logdir}/${i}.log &
                j=0
                runnodes=''
        fi
done
```

## 6.7 Bios设置

````shell
琦哥的：
    UPI Configuration
        Link Lop -> disable
        Link L1  -> disable 
        IO Directory Cache -> enable
        Isoc Mode -> enable

    Memory Configuration
        Eufsrce POR -> enable
        PPR Type -> Soft PPR
        Memory Frequency -> 2666
        Data Scrambling for NVMDIMM -> enable
        Data Scrambling for DDR4 -> enable
        Enable AOR -> enable
        Refresh Option -> enable
        Memory RAS
            Memory Rank Sparing -> enable

    IIO Configuration
        PCI-E Port Max Payload Size -> 256B

    CPU P-State
        SpeedStep -> disable/enable?
        PBF 
        Hardware P-State -> enable
        Package c state -> No limit 
	
网上找的系统Bios设置优化
    关闭超线程
    打开EIST
    打开Turbo Mode
    Boot Performance mode设置为max performance
    Energy Performance BIAS设置为Performance
    打开Monitor/Mwait
    Package C stat limit设置为C0/C1 state
    关闭CPU C3 report
    关闭CPU C6 report
    关闭Enhanced Halt State
    关闭Intel VT for Directed I/O
    Linux OS下CPU Power Management设置为max performance
    QPI及Memory Frequency保持为Max Frequency
    关闭NUMA功能 
````

## 6.8 <span id="jump">结果反馈</span>

```shell
#linpack测试以我们为主导

1、grep FAILED 关键字,查看是否有节点计算错误
2、grep WR 查看节点性能是否正常
3、统计跑死的点

# 如何反馈？
一轮Linpack40分钟测试完成
	一、6个点本是drain*状态
	二、3个点跑死，cn[1,2,3]
	三、5个点Linpack计算FAILED
	四、其他点linpakc性能正常
可以考虑更换下一批
```

## 6.9 风冷系统linpack测试全过程记录

>##### 1、了解风冷系统的架构
>
>按批次测试，测完一批换下一批
>
>一批为192个节点，每个框32个节点，共6个框
>
>每个节点128G内存，芯片为FT2000+, 通过mhz获取频率为2000（降频），64物理核
>
>系统版本： 4.19.46-cn+
>
>##### 2、计算每个节点的理论峰值
>
>2000 * 128 / 64 * 2 * 64= 512000
>
>##### 3、开始测试
>
>runpro.sh脚本参数说明:
>
>```shell
>./runpro.sh NodeList NodeBind ProcessesPerNode MemSize
>NodeList 节点列表,例如cn[0-8]
>NodeBind 几个节点绑定运行，例如2，则计算效率时，需要将结果得到的效率/2/512
>ProcessesPerNode 每给节点运行几个xhpl进程，根据numa node来，FT一般是8
>MemSize 每个节点中，每个进程分配的内存，一般使用到约总内存的80%，例如单节点128G，填写12000（单位为Mb），则总使用96G内存
>```
>
>单节点测试：
>
>​	不需要互联测试网络联通性，仅对单节点进行压力测试
>
>```shell
>./runpro.sh cn0 1 8 12000 logdir
>```
>
>多节点测试：
>
>​	需要互联测试网络联通性， 2，4， 8， 16， 32， 64， 128，如果要求测多节点稳定性，测试的8点8进8线，结果按[章节6.8](#jump)反馈
>
>```shell
>./runpro.sh cn0 1 8 12000 logdir
>```
>
>问题解决一：
>
>​	测试到128节点，性能异常，效率仅有43%
>
>互联那边回应：
>
>​	节点数超过64，通信就跨框
>
>琦哥建议的做法：
>
>​	把128个节点分到4个框，测试一下，看是不是带宽的原因
>
>最后的解决方法：
>
>​	网络拓扑更换，矩阵值不能相等，使用原本32 x 32，改为16 x 64，但是换成160节点后还是有问题
>
>```shell
>./runpro.sh cn[xx-xx] 128 8 12000 dir
>其中执行的xhplrun.sh的参数为
>xhplrun.sh 32 32 12000
>```
>
>

# 七、Lmbench 测试

## 7.1 概述

​	Lmbench是一套简易，可移植的，符合ANSI/C标准为UNIX/POSIX而制定的微型测评工具。一般来说，它衡量两个关键特征：反应时间和带宽。LmBench旨在使系统开发者深入了解关键操作的基础成本。

## 7.2 工具安装

获取源包：

​	lmbench3.tar.gz

gcc编译安装

## 7.3 测试步骤

```shell
cd src
make results
make see
内存10G。
```

```shell
错误处理：
[root@cn9 lmbench3]# cd src/
[root@cn9 src]# make results
gmake[1]: Entering directory `/home/testcpu/test652/tools/lmbench3/src'
gmake[1]: *** No rule to make target `../SCCS/s.ChangeSet', needed by `bk.ver'.  Stop.
gmake[1]: Leaving directory `/home/testcpu/test652/tools/lmbench3/src'
解决方法：
	vim Makefile
	231行修改：
	$O/lmbench : ../scripts/lmbench bk.ver
	改为
	$O/lmbench : ../scripts/lmbench
```

# 八、IOR 测试（暂无）

# 九、alltoall测试

```shell
yhrun -n 128 -N 8 -w cn34,cn35,cn36,cn37,cn38,cn39,cn40,cn41, /root/xm_alltoall 102400 1000
```

# 附录：常用工具链编译方法

## 一、GCC编译

需要依次编译依赖软件，然后在编译gcc中使用

```shell
# 设置isl的库路径
export LD_LIBRARY_PATH=/home/testcpu/test652/tools/gcc830/lib:$LD_LIBRARY_PATH
# 编译GCC
../configure --prefix=/home/testcpu/test652/tools/gcc830 --enable-lto --disable-bootstrap --enable-languages=c,c++,fortran --with-gmp=/home/testcpu/test652/tools/gcc830 --with-mpfr=/home/testcpu/test652/tools/gcc830 --with-mpc=/home/testcpu/test652/tools/gcc830 --with-isl=/home/testcpu/test652/tools/gcc830
```

### 1.1 依赖软件编译

#### 1.1.2 编译安装gmp

```shell
./configure --prefix=/home/testcpu/test652/tools/gcc830
make 
make install

./configure --with-gmp=/home/testcpu/test652/tools/gcc83 --with-mpfr=/home/testcpu/test652/tools/gcc83  --prefix=/home/testcpu/test652/tools/gcc83
```

#### 1.1.3 编译安装mpfr

```shell
./configure --prefix=/home/testcpu/test652/tools/gcc830 --with-gmp=/home/testcpu/test652/tools/gcc830
make 
make install
```

#### 1.1.4 编译安装mpc

```shell
./configure --prefix=/home/testcpu/test652/tools/gcc830 --with-gmp=/home/testcpu/test652/tools/gcc830 --with-mpfr=/home/testcpu/test652/tools/gcc830
make
make install
```

#### 1.1.5 编译安装isl

```shell
./configure --prefix=/home/testcpu/test652/tools/gcc830 --with-gmp-prefix=/home/testcpu/test652/tools/gcc830
```

```shell
 yum install gcc-* gtk2 gtk3 x11 libX11.x86_64 libX11-devel.x86_64 libXorg libXss libXScrnSaver.x86_64 xorg-x11-server-Xorg.x86_64 xulrunner.x86_64 xulrunner.i686 libstdc++.i686 libstdc++-devel.i686
glibc.i686 glibc-devel.i686 libgcc* xulrunner.x86_64 xulrunner.i686 glibc-devel.x86_64 glibc.x86_64 autoconf-archive.noarch gtk2 gtk3 pango libXScrnSaver libX11.x86_64 libX11-devel.x86_64 libX11-common.noarch libxkbcommon-x11.x86_64 xorg-x11-server-common.x86_64 libstdc++* -y
```

```shell
#!/bin/bash
mount -o loop -t iso9660 /home/testcpu/test652/images/rhel-server-7.6-x86_64-dvd.iso /home/testcpu/test652/mnt/CDROM


yum install gcc-* gtk2 gtk3 glibc-devel.i686 x11 libX11.x86_64 libX11-devel.x86_64 libXorg libXss libXScrnSaver.x86_64 xorg-x11-server-Xorg.x86_64 xulrunner.x86_64 xulrunner.i686 libstdc++.i686 libstdc++-devel.i686 glibc* libgcc* xulrunner* autoconf-archive.noarch gtk2 gtk3 pango libXScrnSaver libX11.x86_64 libX11-devel.x86_64 libX11-common.noarch libxkbcommon-x11.x86_64 xorg-x11-server-common.x86_64 libstdc++* -y

```

## 二、MPICH3编译

设置gcc830的环境变量

```shell
export PATH=/home/testcpu/test652/tools/gcc830/bin:$PATH
export LD_LIBRARY_PATH=/home/testcpu/test652/tools/gcc830/lib:$LD_LIBRARY_PATH
export LD_INCLUDE_PATH=/home/testcpu/test652/tools/gcc830/include:$LD_INCLUDE_PATH
export CPATH=/home/testcpu/test652/tools/gcc830/include:$CPATH
export LD_LIBRARY_PATH=/home/testcpu/test652/tools/gcc830/lib64:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/home/testcpu/test652/tools/gcc830/libexec:$LD_LIBRARY_PATH
```

再编译

```shell
./configure --prefix=/home/testcpu/test652/tools/mpi3-gcc830 --enable-fast --enable-shared=yes --enable-threads=runtime --with-ch3-rank-bits=32 --enable-romio --with-file-system=ufs+nfs --with-mpe
make 
make install
```

设置环境变量使用

```shell
export PATH=/home/testcpu/test652/tools/mpi3-gcc830/bin:$PATH
export LD_LIBRARY_PATH=/home/testcpu/test652/tools/mpi3-gcc830/lib:$LD_LIBRARY_PATH
export LD_INCLUDE_PATH=/home/testcpu/test652/tools/mpi3-gcc830/include:$LD_INCLUDE_PATH
```



### 三、Redhat设置本地镜像源

```shell
 mount -o loop -t iso9660 /home/IOSYS/test652/images/rhel-server-7.6-x86_64-dvd.iso /home/IOSYS/test652/mnt/CDROM
```

```shell
vim /etc/yum.repos.d/redhat.repo

[rhel7.6]
name=rhel7.6
baseurl=file:///home/testcpu/test652/mnt/CDROM
enabled=1
gpgcheck=0
priority=1

yum clean
yum update
yum repolist
```

## 四、找不到so库的解决方法

```shell
# 方法一：
	设置LD_LIBRARY_PATH
# 方法二：
 	vim /etc/ld.so.conf
 	----------------------------------------------------
 	include ld.so.conf.d/*.conf
	/opt/intel/compilers_and_libraries/linux/lib/intel64
	ldconfig 更新
```

## 五、复制以及查看端口占用

```shell
# 查看端口占用
netstat -nultp | grep 8080
netstat -anp  | grep 80
netstat -ano | grep 18130
lsof -i:18130

# 复制
rsync -chavP /var/opt/gitlab/postgresql .
rsync -avzP 复制显示进度

# 查看Bios版本
dmidecode
```

```shell

mpiicc -DAdd__ -DF77_INTEGER=int -DStringSunStyle -DHPL_DETAILED_TIMING -DHPL_PROGRESS_REPORT -I/root/hpl/include -I/root/hpl/include/Linux_Intel64 -I/opt/intel/compilers_and_libraries_2019.4.243/linux/mkl/mkl/include  -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall -qopenmp -mt_mpi -o /root/hpl/bin/Linux_Intel64/xhpl HPL_pddriver.o         HPL_pdinfo.o           HPL_pdtest.o /root/hpl/lib/Linux_Intel64/libhpl.a  -L/opt/intel/compilers_and_libraries_2019.4.243/linux/mkl/mkl/lib/intel64 -Wl,--start-group /opt/intel/compilers_and_libraries_2019.4.243/linux/mkl/lib/intel64/libmkl_intel_lp64.a /opt/intel/compilers_and_libraries_2019.4.243/linux/mkl/lib/intel64/libmkl_intel_thread.a /opt/intel/compilers_and_libraries_2019.4.243/linux/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lpthread -ldl
```

##  六、查看cpu主频率 和 内存信息

```shell
# 查看CPU信息
lscpu 其中 sockets个数代表物理核个数
cat /proc/cpuinfo
lmbench的mhz
/usr/bin/turbostat

# 查看内存信息
cat /proc/meminfo
dmidecode | grep -A 16 "Memory Device"、
```

## 七、GLSL 1.50 is not supported

```shell
# 报错：
ERROR: In /home/gs/src/VTK8.2/VTK-8.2.0/Rendering/OpenGL2/vtkShaderProgram.cxx, line 447
vtkShaderProgram (0x2b87270): 0:1(10): error: GLSL 1.50 is not supported. Supported versions are: 1.10, 1.20, 1.30, 1.00 ES, and 3.00 ES

# 解决方法
export MESA_GL_VERSION_OVERRIDE=3.2
```

```shell
cn[16-19,24-27,144-147,152-155]
```

## 八、附录

### 8.1 Linpack脚本

> runpro.sh
>
> ./runpro.sh nodes_list nodes_num proc_per_node mem_per_proc logdir
>
> node_list 节点列表
>
> nodes_num 几个点连在一起跑，单点 双点 多点
>
>  proc_per_node 每个节点跑几个进程，看node分为几个，FT一般是8
>
>  mem 12000 每个进程分配的内存，*8 大概= 总内存的80%
>
>  logdir 日志文件
>
>  单点8进程，一轮≈35分钟

```shell
#! /bin/bash

#duqi

if [ $# -ne 5 ]
then
        echo Usage: ./runpro.sh nodes_list nodes_num proc_per_node mem_per_proc logdir
        exit 0
fi

export HPL_CMDLINE=1
export LD_LIBRARY_PATH=/usr/local/mpi3/lib:LD_LIBRARY_PATH

nodes=${1}
nnodes=${2}
nprocs=${3}
nmem=${4}
logdir=${5}


mkdir -p ${logdir}

tnprocs=$((${nnodes}*${nprocs}))
P=1
Q=1


num_max=1
for i in `seq 1 ${tnprocs}`
do
        j=$((${i}*${i}))
        if [ ${tnprocs} -le ${j} ]; then
                num_max=${i}
                break
        fi
done
for i in `seq 1 ${num_max}`
do
        for j in `seq ${num_max} ${tnprocs}`
        do
                PQ=$((${i}*${j}))
                if [ ${PQ} -eq ${tnprocs} ]
                then
                        P=${i}
                        Q=${j}
                fi
        done
done


for i in `yhcontrol show hostname ${nodes}`
do
        scp HPL.dat $i:/root/linpack &
done
sleep 1

j=0
runnodes=''

for i in `yhcontrol show hostname ${nodes}`
do
        runnodes+=${i},
        let j++
        if [ ${j} -eq ${nnodes} ]
        then
                echo "yhrun -p All -N ${nnodes} -n ${tnprocs} -w ${runnodes} -D /root/linpack /root/linpack/xhplrun.sh ${P} ${Q} ${nmem} &>> ${logdir}/${i}.log &"
                echo ${runnodes} &>> ${logdir}/${i}.log &
                echo "yhrun -p All -N ${nnodes} -n ${tnprocs} -w ${runnodes} -D /root/linpack /root/linpack/xhplrun.sh ${P} ${Q} ${nmem}" &>> ${logdir}/list.log
                yhrun -p All -N ${nnodes} -n ${tnprocs} -w ${runnodes} -D /root/linpack /root/linpack/xhplrun.sh ${P} ${Q} ${nmem} &>> ${logdir}/${i}.log &
                j=0
                runnodes=''
        fi
done

```

>xhplrun.sh

```shell
#! /bin/bash

#duqi

ulimit -s unlimited

Cores=64
#==========================================================
PMI_SIZE=$SLURM_NPROCS

PMI_RANK=$SLURM_PROCID

MPI_NUM_NODES=$SLURM_NNODES

MPI_PER_NODE=$((PMI_SIZE / MPI_NUM_NODES))

MPI_RANK_FOR_NODE=$((PMI_RANK % MPI_PER_NODE))
#==========================================================


threads=`echo ${Cores}*${SLURM_NNODES}/${PMI_SIZE} | bc`

PRO_SIZE=${MPI_PER_NODE}
PMI_RANK_my=${MPI_RANK_FOR_NODE}

echo ${PRO_SIZE} ${PMI_RANK_my} ${threads} ${PMI_SIZE} ${PMI_RANK} ${MPI_NUM_NODES} ${MPI_PER_NODE} ${MPI_RANK_FOR_NODE}

export HPL_CMDLINE=1
case ${PRO_SIZE} in
1)
#numactl -i 0-1 -N 0-1 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 0,4 -N 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0-63'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
2)
case ${PMI_RANK_my} in
0)
#numactl -i 0-3 -N 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -m 3 -N 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0-31'
numactl -i 4-7 -N 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
1)
#numactl -i 4-7 -N 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -m 7 -N 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='32-63'
numactl -i 0-3 -N 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
esac
#mpirun -n 1 numactl -i 0-3 -N 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3} : -n 1 numactl -i 4-7 -N 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
4)
case ${PMI_RANK_my} in
0)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0-15'
numactl -i 0-7 -N 0-1 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 2-3 -N 0-1 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
1)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='16-31'
numactl -i 0-7 -N 2-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 4-5 -N 2-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
2)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='32-47'
numactl -i 0-7 -N 4-5 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 6-7 -N 4-5 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
3)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='48-63'
numactl -i 0-7 -N 6-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 0-1 -N 6-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
esac
;;
8)
case ${PMI_RANK_my} in
0)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0-7'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 4 -N 0 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
1)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='8-15'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 2 -N 1 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
2)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='16-23'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 6 -N 2 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
3)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='24-31'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 1 -N 3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
4)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='32-39'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 0 -N 4 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
5)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='40-47'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 7 -N 5 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
6)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='48-55'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 5 -N 6 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
7)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='56-63'
numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#numactl -i 3 -N 7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
esac
;;
16)
case ${PMI_RANK_my} in
0)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0-3'
numactl -i 0-7 -C 0-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
1)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='4-7'
numactl -i 0-7 -C 4-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
2)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='8-11'
numactl -i 0-7 -C 8-11 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
3)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='12-15'
numactl -i 0-7 -C 12-15 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
4)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='16-19'
numactl -i 0-7 -C 16-19 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
5)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='20-23'
numactl -i 0-7 -C 20-23 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
6)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='24-27'
numactl -i 0-7 -C 24-27 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
7)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='28-31'
numactl -i 0-7 -C 28-31 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
8)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='32-35'
numactl -i 0-7 -C 32-35 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
9)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='36-39'
numactl -i 0-7 -C 36-39 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
10)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='40-43'
numactl -i 0-7 -C 40-43 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
11)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='44-47'
numactl -i 0-7 -C 44-47 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
12)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='48-51'
numactl -i 0-7 -C 48-51 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
13)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='52-55'
numactl -i 0-7 -C 52-55 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
14)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='56-59'
numactl -i 0-7 -C 56-59 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
15)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='60-63'
numactl -i 0-7 -C 60-63 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
esac
;;
32)
#export OMP_NUM_THREADS=${threads}
#numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#;;
case ${PMI_RANK_my} in
0)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0-1'
numactl -i 0-7 -C 0-1 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
1)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='2-3'
numactl -i 0-7 -C 2-3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
2)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='4-5'
numactl -i 0-7 -C 4-5 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
3)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='6-7'
numactl -i 0-7 -C 6-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
4)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='8-9'
numactl -i 0-7 -C 8-9 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
5)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='10-11'
numactl -i 0-7 -C 10-11 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
6)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='12-13'
numactl -i 0-7 -C 12-13 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
7)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='14-15'
numactl -i 0-7 -C 14-15 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
8)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='16-17'
numactl -i 0-7 -C 16-17 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
9)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='18-19'
numactl -i 0-7 -C 18-19 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
10)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='20-21'
numactl -i 0-7 -C 20-21 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
11)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='22-23'
numactl -i 0-7 -C 22-23 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
12)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='24-25'
numactl -i 0-7 -C 24-25 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
13)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='26-27'
numactl -i 0-7 -C 26-27 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
14)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='28-29'
numactl -i 0-7 -C 28-29 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
15)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='30-31'
numactl -i 0-7 -C 30-31 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
16)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='32-33'
numactl -i 0-7 -C 32-33 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
17)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='34-35'
numactl -i 0-7 -C 34-35 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
18)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='36-37'
numactl -i 0-7 -C 36-37 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
19)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='38-39'
numactl -i 0-7 -C 38-39 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
20)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='40-41'
numactl -i 0-7 -C 40-41 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
21)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='42-43'
numactl -i 0-7 -C 42-43 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
22)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='44-45'
numactl -i 0-7 -C 44-45 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
23)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='46-47'
numactl -i 0-7 -C 46-47 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
24)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='48-49'
numactl -i 0-7 -C 48-49 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
25)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='50-51'
numactl -i 0-7 -C 50-51 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
26)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='52-53'
numactl -i 0-7 -C 52-53 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
27)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='54-55'
numactl -i 0-7 -C 54-55 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
28)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='56-57'
numactl -i 0-7 -C 56-57 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
29)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='58-59'
numactl -i 0-7 -C 58-59 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
30)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='60-61'
numactl -i 0-7 -C 60-61 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
31)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='62-63'
numactl -i 0-7 -C 62-63 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
esac
;;
64)
export OMP_NUM_THREADS=${threads}
#export GOMP_CPU_AFFINITY='0-63'
#numactl -i 0-7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
#;;
case ${PMI_RANK_my} in
0)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='0'
numactl -i 0-7 -C 0 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
1)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='1'
numactl -i 0-7 -C 1 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
2)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='2'
numactl -i 0-7 -C 2 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
3)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='3'
numactl -i 0-7 -C 3 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
4)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='4'
numactl -i 0-7 -C 4 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
5)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='5'
numactl -i 0-7 -C 5 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
6)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='6'
numactl -i 0-7 -C 6 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
7)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='7'
numactl -i 0-7 -C 7 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
8)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='8'
numactl -i 0-7 -C 8 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
9)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='9'
numactl -i 0-7 -C 9 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
10)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='10'
numactl -i 0-7 -C 10 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
11)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='11'
numactl -i 0-7 -C 11 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
12)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='12'
numactl -i 0-7 -C 12 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
13)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='13'
numactl -i 0-7 -C 13 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
14)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='14'
numactl -i 0-7 -C 14 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
15)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='15'
numactl -i 0-7 -C 15 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
16)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='16'
numactl -i 0-7 -C 16 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
17)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='17'
numactl -i 0-7 -C 17 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
18)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='18'
numactl -i 0-7 -C 18 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
19)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='19'
numactl -i 0-7 -C 19 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
20)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='20'
numactl -i 0-7 -C 20 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
21)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='21'
numactl -i 0-7 -C 21 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
22)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='22'
numactl -i 0-7 -C 22 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
23)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='23'
numactl -i 0-7 -C 23 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
24)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='24'
numactl -i 0-7 -C 24 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
25)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='25'
numactl -i 0-7 -C 25 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
26)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='26'
numactl -i 0-7 -C 26 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
27)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='27'
numactl -i 0-7 -C 27 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
28)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='28'
numactl -i 0-7 -C 28 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
29)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='29'
numactl -i 0-7 -C 29 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
30)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='30'
numactl -i 0-7 -C 30 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
31)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='31'
numactl -i 0-7 -C 31 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
32)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='32'
numactl -i 0-7 -C 32 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
33)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='33'
numactl -i 0-7 -C 33 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
34)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='34'
numactl -i 0-7 -C 34 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
35)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='35'
numactl -i 0-7 -C 35 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
36)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='36'
numactl -i 0-7 -C 36 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
37)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='37'
numactl -i 0-7 -C 37 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
38)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='38'
numactl -i 0-7 -C 38 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
39)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='39'
numactl -i 0-7 -C 39 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
40)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='40'
numactl -i 0-7 -C 40 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
41)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='41'
numactl -i 0-7 -C 41 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
42)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='42'
numactl -i 0-7 -C 42 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
43)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='43'
numactl -i 0-7 -C 43 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
44)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='44'
numactl -i 0-7 -C 44 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
45)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='45'
numactl -i 0-7 -C 45 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
46)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='46'
numactl -i 0-7 -C 46 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
47)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='47'
numactl -i 0-7 -C 47 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
48)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='48'
numactl -i 0-7 -C 48 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
49)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='49'
numactl -i 0-7 -C 49 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
50)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='50'
numactl -i 0-7 -C 50 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
51)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='51'
numactl -i 0-7 -C 51 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
52)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='52'
numactl -i 0-7 -C 52 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
53)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='53'
numactl -i 0-7 -C 53 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
54)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='54'
numactl -i 0-7 -C 54 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
55)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='55'
numactl -i 0-7 -C 55 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
56)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='56'
numactl -i 0-7 -C 56 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
57)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='57'
numactl -i 0-7 -C 57 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
58)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='58'
numactl -i 0-7 -C 58 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
59)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='59'
numactl -i 0-7 -C 59 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
60)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='60'
numactl -i 0-7 -C 60 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
61)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='61'
numactl -i 0-7 -C 61 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
62)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='62'
numactl -i 0-7 -C 62 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
63)
export OMP_NUM_THREADS=${threads}
export GOMP_CPU_AFFINITY='63'
numactl -i 0-7 -C 63 ./xhpl -n 1 -b 192 -p ${1} -q ${2} -m ${3}
;;
esac
;;
esac
```

### 8.2 编译问题

```shell
-lc 
	解决找不到memcpy问题 
-lstdc++
	解决vtable for xxxx 问题
-lgcc_s
	undefined reference to `_Unwind_Resume'
	undefined reference to `__popcountdi2'
	
#include <sys/types.h>
#include <sys/types.h>
	 implicit declaration of function ‘fstat’

#include <sys/sysmacros.h>
	implicit declaration of function ‘major’
```

# 十、Linux操作

## 10.1 ncat端口转发

```shell
ncat --sh-exec "ncat 25.8.27.6 15929" -l 15929 --keep-open
```

# 十一、DDT安装

```shell
# cp Licence /home/gs/tools/arm/forge/licences/
# cp Licence.14713 /home/gs/tools/arm/licenceserver/licences/

export PATH=/opt/mpi3.3/bin:/home/gs/tools/arm/forge/bin:/home/gs/tools/arm/licenceserver/bin:$PATH
export LD_LIBRARY_PATH=/opt/mpi3.3/lib:$LD_LIBRARY_PATH
ifconfig eth3 hw ether 00:1b:21:14:15:60
```

```shell
#glex高速网版本的MPI改为本地调试用
export MPICH_NO_LOCAL=0 //？不一定需要
export MPICH_NEMESIS_NETMOD=tcp
export PATH=/usr/local/mpi3/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/mpi3/lib:$LD_LIBRARY_PATH
```



# 十二、SPEC使用

## 12.1 SPEC使用系统工具

```shell
# SEPC设置使用系统的工具
vim /home/gs/tools/spack-v5/etc/spack/defaults/packages.yaml
# 参考：
../../../lib/spack/docs/build_settings.rst
```

# 十三、mi协议

## 13.1 使用mi启动调试

```shell
gs@ft-svr:~/matrix$ gdb --interpreter mi test
=thread-group-added,id="i1"
~"GNU gdb (Ubuntu 8.2.91.20190405-0ubuntu3) 8.2.91.20190405-git\n"
~"Copyright (C) 2019 Free Software Foundation, Inc.\n"
~"License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law."
~"\nType \"show copying\" and \"show warranty\" for details.\n"
~"This GDB was configured as \"aarch64-linux-gnu\".\n"
~"Type \"show configuration\" for configuration details.\n"
~"For bug reporting instructions, please see:\n"
~"<http://www.gnu.org/software/gdb/bugs/>.\n"
~"Find the GDB manual and other documentation resources online at:\n    <http://www.gnu.org/software/gdb/documentation/>."
~"\n\n"
~"For help, type \"help\".\n"
~"Type \"apropos word\" to search for commands related to \"word\"...\n"
~"Reading symbols from test...\n"
(gdb)
```

## 13.2 断点命令（BreakPoint）

### 13.2.1 -break-after

```shell
-break-after number count
第number个断点在执行count次后有效
```

### 13.2.2 -break-condition

```shell
-break-condition number expr
第number个断点在表达式expr为true时有效
```

### 13.2.3 -break-delete

```shell
-break-delete(breakpoint number)+
删除指定number的多个断点
```

### 13.2.4 -break-disable

```shell
-break-disable(breakpoint number)+
使用指定Number的多个断点失效
```

### 13.2.5 -break-enable

```shell
-break-enable(breakpoint number)+
使用指定Number的多个断点生效
```

### 13.2.6 -break-info

```shell
-break-info breakpoint
得到指定断点的信息
```

### 13.2.7 -break-insert

```shell
-break-insert
	-t				插入临时断点
	-h				插入硬件断点
	-r				插入正则断点，当函数名匹配正则表达式时生效
	-c condition    插入条件断点
	-i ignore-count 插入一个指定无效次数的断点
	-p thread  		
	line | addr(func) 
```

### 13.2.8. -break-list

```shell
-break-list
显示已插入的断点列表
```

### 13.2.9 -break-watch

```shell
-break-watch [-a|-r] variable
创建一个观察点，-a标识对variable读写时有效，-r标识只读时有效
```

## 13.3 程序环境命令(Program Context)

## 13.4 线程(thread)

## 13.5 程序执行(Program Execution)



## 13.6 栈(Stack)

## 13.7 变量(Variable)

## 13.8 数据(Data)

## 13.9 跟踪点(TracePoint)

## 13.10 符号(Symbol)

## 13.11 文件(File)

## 13.12 目标数据(Target Manipulation)

## 13.13 其他

```shell
-enable-pretty-printing
-var-create - * map
-var-list-children --all-values var1 0 10

-data-evaluate-expression map
^done,value="std::map with 140737353945088 elements<error reading variable: Cannot access memory at address 0x1f0fc35f415e51>"
```



```shell
//    // 设置标记类型与颜色
//    this->markerDefine(QsciScintilla::Circle, 0);
//    this->setMarkerBackgroundColor(QColor("#ee6666"), 0);
//    this->markerDefine(QsciScintilla::Circle, 1);
//    this->setMarkerBackgroundColor(QColor("#aaaaaa"), 1);
//    this->markerDefine(QsciScintilla::RightArrow, 2);
//    this->setMarkerBackgroundColor(QColor("#eaf593"), 2);
```

# 十四、Git改密码

```shell
gitlab-rails console production

irb(main):002:0> user=User.where(email:'cuiyingbomail@163.com').first
=> #<User id:30 @cyb>
irb(main):003:0> user.password=12345678
=> 12345678
irb(main):004:0> user.password_confirmation=12345678
=> 12345678
irb(main):005:0> user.save!
Enqueued ActionMailer::DeliveryJob (Job ID: 540244db-c6ec-44f9-880e-72338b955aa5) to Sidekiq(mailers) with arguments: "DeviseMailer", "password_change", "deliver_now", #<GlobalID:0x00007f57f5da3208 @uri=#<URI::GID gid://gitlab/User/30>>
=> true

```

# 十五、YHPDE（eclipse）FT部署

```shell
1、安装eclipse:
	gitlab
2、插件安装，参考手册
2、新版SLURM需要
vim org.eclipse.ptp.rm.slurm.proxy_4.0.7.201104291906/src/ptp_slurm_proxy.c
1625行：slurm_allocation_lookup_lite -> slurm_allocation_lookup
2942行: primary = 1; ->  primary = 0;
2943行：secondary = 2; -> secondary = 1;
```

